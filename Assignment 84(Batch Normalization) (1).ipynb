{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745bb31-8386-4fea-8d15-5bd174a26fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "Objective: The objective of this assignment is to assess students understanding of batch normalization in artificial neural networks (ANN) \n",
    "           and its impact on training performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45381630-94c4-4dc5-a871-eab1d96cee1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. TheTry and CTnceptsU\n",
    "\n",
    "1. Explain the concept of batch normalization in the context of Artificial Neural Networksr\n",
    "\n",
    "ANS- Batch normalization is a technique used in artificial neural networks to normalize the inputs of each layer during training. It \n",
    "     involves adjusting and scaling the activations within a mini-batch of training examples to have zero mean and unit variance. \n",
    "     The purpose of batch normalization is to address the problem of internal covariate shift, where the distribution of inputs to each \n",
    "    layer changes as the network learns.\n",
    "    \n",
    "\n",
    "2. Describe the benefits of using batch normalization during trainingr\n",
    "\n",
    "ANS- Using batch normalization during training offers several benefits:\n",
    "\n",
    "1. Improved training speed: Batch normalization helps accelerate the training process by reducing the internal covariate shift. It allows \n",
    "                            for more stable and faster convergence of the network.\n",
    "2. Robustness to initialization: Batch normalization reduces the sensitivity of the network to the initial parameter values. It helps \n",
    "                                 prevent the network from getting stuck in poor local optima during optimization.\n",
    "3. Regularization effect: Batch normalization introduces a slight regularization effect, reducing the need for other regularization \n",
    "                          techniques like dropout or L2 regularization.\n",
    "4. Generalization: By normalizing the inputs within each batch, batch normalization reduces the reliance of the network on specific \n",
    "                   training examples. It promotes better generalization to unseen data.\n",
    "    \n",
    "    \n",
    "3. Discuss the working principle of batch normalization, including the normalization step and the learnable parameters.\n",
    "\n",
    "ANS- The working principle of batch normalization involves two key steps: normalization and learnable parameters.\n",
    "\n",
    "Normalization step:\n",
    "\n",
    "1. In the normalization step, for each feature dimension, the activations within a mini-batch are normalized to have zero mean and unit \n",
    "   variance. This is achieved by subtracting the mini-batch mean and dividing by the mini-batch standard deviation.\n",
    "2. The normalization step helps in reducing the internal covariate shift and ensures that the inputs to each layer are roughly in the same \n",
    "   range, making the optimization process more stable.\n",
    "\n",
    "\n",
    "Learnable parameters:\n",
    "\n",
    "1. In addition to the normalization step, batch normalization introduces learnable parameters to further improve the network's \n",
    "   representational power.\n",
    "2. Two learnable parameters are introduced: scale and shift. The scale parameter allows the network to rescale the normalized values, and the \n",
    "   shift parameter allows the network to adjust the mean.\n",
    "3. The scale and shift parameters are learned during training using backpropagation, and they provide the flexibility to the network to learn \n",
    "   the optimal scaling and shifting of the normalized values.\n",
    "\n",
    "\n",
    "The normalization and learnable parameters of batch normalization are applied independently for each feature dimension (i.e., for each \n",
    "neuron) within a layer. The process is repeated for each mini-batch during training.\n",
    "\n",
    "Overall, batch normalization helps in stabilizing and accelerating the training process by normalizing the inputs within each mini-batch \n",
    "and introducing learnable parameters for further flexibility. It promotes better convergence, generalization, and robustness in artificial \n",
    "neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18f5a490-5825-4374-9b58-26a11a38996e",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. Impementation\n",
    "    1.  Choose a dataset of your choice (e.g., MNIST, CIAR-0) and preprocess it.\n",
    "    2. Implement a simple feedforward neural network using any deep learning framework/library (e.g.,Tensorflow, PyTorch)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47e9b543-3883-477e-b8d7-19fa74e4a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. For this example, lets choose the MNIST dataset, which consists of handwritten digit images. Preprocessing steps typically include:\n",
    "\n",
    "    1. Loading the dataset.\n",
    "    2. Scaling the pixel values to a range between 0 and 1.\n",
    "    3. Splitting the dataset into training and validation sets.\n",
    "    4. Optionally, reshaping or augmenting the data based on the specific requirements of the deep learning framework or network \n",
    "       architecture.\n",
    "        \n",
    "        \n",
    "2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "131e21c0-e613-44b5-bc43-06bf91502fb4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Package(s) not found: tensorflow\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip show tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ab73dbc-3005-45bf-a68f-8ceb7d5579c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Skipping tensorflow as it is not installed.\u001b[0m\u001b[33m\n",
      "\u001b[0mCollecting tensorflow\n",
      "  Downloading tensorflow-2.13.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (524.1 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m524.1/524.1 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting opt-einsum>=2.3.2\n",
      "  Downloading opt_einsum-3.3.0-py3-none-any.whl (65 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m65.5/65.5 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: typing-extensions<4.6.0,>=3.6.6 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.4.0)\n",
      "Collecting astunparse>=1.6.0\n",
      "  Downloading astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Collecting gast<=0.4.0,>=0.2.1\n",
      "  Downloading gast-0.4.0-py3-none-any.whl (9.8 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.32.0-cp310-cp310-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (2.4 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting absl-py>=1.0.0\n",
      "  Downloading absl_py-1.4.0-py3-none-any.whl (126 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.5/126.5 kB\u001b[0m \u001b[31m19.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting grpcio<2.0,>=1.24.3\n",
      "  Downloading grpcio-1.56.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.2/5.2 MB\u001b[0m \u001b[31m70.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting libclang>=13.0.0\n",
      "  Downloading libclang-16.0.0-py2.py3-none-manylinux2010_x86_64.whl (22.9 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m22.9/22.9 MB\u001b[0m \u001b[31m58.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: h5py>=2.9.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (3.7.0)\n",
      "Collecting keras<2.14,>=2.13.1\n",
      "  Downloading keras-2.13.1-py3-none-any.whl (1.7 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting termcolor>=1.1.0\n",
      "  Downloading termcolor-2.3.0-py3-none-any.whl (6.9 kB)\n",
      "Requirement already satisfied: setuptools in /opt/conda/lib/python3.10/site-packages (from tensorflow) (65.5.1)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.10/site-packages (from tensorflow) (22.0)\n",
      "Collecting google-pasta>=0.1.1\n",
      "  Downloading google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m57.5/57.5 kB\u001b[0m \u001b[31m10.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: six>=1.12.0 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.16.0)\n",
      "Collecting wrapt>=1.11.0\n",
      "  Downloading wrapt-1.15.0-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (78 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.4/78.4 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting flatbuffers>=23.1.21\n",
      "  Downloading flatbuffers-23.5.26-py2.py3-none-any.whl (26 kB)\n",
      "Requirement already satisfied: numpy<=1.24.3,>=1.22 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (1.23.5)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /opt/conda/lib/python3.10/site-packages (from tensorflow) (4.21.11)\n",
      "Collecting tensorflow-estimator<2.14,>=2.13.0\n",
      "  Downloading tensorflow_estimator-2.13.0-py2.py3-none-any.whl (440 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m440.8/440.8 kB\u001b[0m \u001b[31m49.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting tensorboard<2.14,>=2.13\n",
      "  Downloading tensorboard-2.13.0-py3-none-any.whl (5.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.6/5.6 MB\u001b[0m \u001b[31m85.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: wheel<1.0,>=0.23.0 in /opt/conda/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow) (0.38.4)\n",
      "Collecting markdown>=2.6.8\n",
      "  Downloading Markdown-3.4.3-py3-none-any.whl (93 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m93.9/93.9 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting google-auth<3,>=1.6.3\n",
      "  Downloading google_auth-2.22.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.8/181.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: requests<3,>=2.21.0 in /opt/conda/lib/python3.10/site-packages (from tensorboard<2.14,>=2.13->tensorflow) (2.28.1)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0\n",
      "  Downloading tensorboard_data_server-0.7.1-py3-none-manylinux2014_x86_64.whl (6.6 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.6/6.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25hCollecting google-auth-oauthlib<1.1,>=0.5\n",
      "  Downloading google_auth_oauthlib-1.0.0-py2.py3-none-any.whl (18 kB)\n",
      "Collecting werkzeug>=1.0.1\n",
      "  Downloading Werkzeug-2.3.6-py3-none-any.whl (242 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.5/242.5 kB\u001b[0m \u001b[31m35.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting cachetools<6.0,>=2.0.0\n",
      "  Downloading cachetools-5.3.1-py3-none-any.whl (9.3 kB)\n",
      "Collecting pyasn1-modules>=0.2.1\n",
      "  Downloading pyasn1_modules-0.3.0-py2.py3-none-any.whl (181 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 kB\u001b[0m \u001b[31m31.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hCollecting rsa<5,>=3.1.4\n",
      "  Downloading rsa-4.9-py3-none-any.whl (34 kB)\n",
      "Requirement already satisfied: urllib3<2.0 in /opt/conda/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.14,>=2.13->tensorflow) (1.26.13)\n",
      "Collecting requests-oauthlib>=0.7.0\n",
      "  Downloading requests_oauthlib-1.3.1-py2.py3-none-any.whl (23 kB)\n",
      "Requirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (3.4)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.14,>=2.13->tensorflow) (2022.12.7)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /opt/conda/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.14,>=2.13->tensorflow) (2.1.1)\n",
      "Collecting pyasn1<0.6.0,>=0.4.6\n",
      "  Downloading pyasn1-0.5.0-py2.py3-none-any.whl (83 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.9/83.9 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: oauthlib>=3.0.0 in /opt/conda/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<1.1,>=0.5->tensorboard<2.14,>=2.13->tensorflow) (3.2.2)\n",
      "Installing collected packages: libclang, flatbuffers, wrapt, werkzeug, termcolor, tensorflow-io-gcs-filesystem, tensorflow-estimator, tensorboard-data-server, pyasn1, opt-einsum, markdown, keras, grpcio, google-pasta, gast, cachetools, astunparse, absl-py, rsa, requests-oauthlib, pyasn1-modules, google-auth, google-auth-oauthlib, tensorboard, tensorflow\n",
      "Successfully installed absl-py-1.4.0 astunparse-1.6.3 cachetools-5.3.1 flatbuffers-23.5.26 gast-0.4.0 google-auth-2.22.0 google-auth-oauthlib-1.0.0 google-pasta-0.2.0 grpcio-1.56.0 keras-2.13.1 libclang-16.0.0 markdown-3.4.3 opt-einsum-3.3.0 pyasn1-0.5.0 pyasn1-modules-0.3.0 requests-oauthlib-1.3.1 rsa-4.9 tensorboard-2.13.0 tensorboard-data-server-0.7.1 tensorflow-2.13.0 tensorflow-estimator-2.13.0 tensorflow-io-gcs-filesystem-0.32.0 termcolor-2.3.0 werkzeug-2.3.6 wrapt-1.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip uninstall tensorflow\n",
    "!pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "eb219889-f72c-4871-82ef-0d28cc8d2571",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-07-14 09:07:15.908752: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-14 09:07:15.971994: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-07-14 09:07:15.974351: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-07-14 09:07:17.156595: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
      "11490434/11490434 [==============================] - 2s 0us/step\n",
      "Epoch 1/10\n",
      "1875/1875 [==============================] - 6s 3ms/step - loss: 0.2533 - accuracy: 0.9283 - val_loss: 0.1312 - val_accuracy: 0.9623\n",
      "Epoch 2/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.1109 - accuracy: 0.9674 - val_loss: 0.0959 - val_accuracy: 0.9692\n",
      "Epoch 3/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0754 - accuracy: 0.9774 - val_loss: 0.0924 - val_accuracy: 0.9718\n",
      "Epoch 4/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0554 - accuracy: 0.9829 - val_loss: 0.0866 - val_accuracy: 0.9731\n",
      "Epoch 5/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0442 - accuracy: 0.9863 - val_loss: 0.0829 - val_accuracy: 0.9754\n",
      "Epoch 6/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0339 - accuracy: 0.9901 - val_loss: 0.0746 - val_accuracy: 0.9774\n",
      "Epoch 7/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0266 - accuracy: 0.9918 - val_loss: 0.0754 - val_accuracy: 0.9795\n",
      "Epoch 8/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0212 - accuracy: 0.9933 - val_loss: 0.0879 - val_accuracy: 0.9777\n",
      "Epoch 9/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0187 - accuracy: 0.9943 - val_loss: 0.0861 - val_accuracy: 0.9783\n",
      "Epoch 10/10\n",
      "1875/1875 [==============================] - 5s 3ms/step - loss: 0.0147 - accuracy: 0.9956 - val_loss: 0.0830 - val_accuracy: 0.9793\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.History at 0x7ff5f48c9720>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and preprocess the MNIST dataset\n",
    "(x_train, y_train), (x_val, y_val) = tf.keras.datasets.mnist.load_data()\n",
    "x_train = x_train / 255.0\n",
    "x_val = x_val / 255.0\n",
    "\n",
    "# Define the model architecture\n",
    "model = tf.keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam',\n",
    "              loss='sparse_categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# Train the model without batch normalization\n",
    "model.fit(x_train, y_train, epochs=10, validation_data=(x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "809ebbdb-4ef1-4a82-aad8-6a0c6b08626d",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Train the neural network on the chosen dataset without using batch normalizationr\n",
    "\n",
    "ANS- By using the code snippet provided above, the neural network will be trained on the MNIST dataset without using batch normalization. \n",
    "     The model will be trained for 10 epochs, and the training and validation performance metrics (such as accuracy and loss) will be \n",
    "     recorded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a4616d8-f44d-4b7b-b0a3-6c916c1fae16",
   "metadata": {},
   "outputs": [],
   "source": [
    "4. Implement batch normalization layers in the neural network and train the model again.\n",
    "\n",
    "ANS- With the addition of the BatchNormalization layer after the first Dense layer, the model now includes batch normalization.\n",
    "\n",
    "Train the model again using the same training and validation data, and record the training and validation performance metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8c896115-afb3-4145-a32b-53a2620a1d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "    layers.Flatten(input_shape=(28, 28)),\n",
    "    layers.Dense(128),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(10, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa74d636-5de2-49b1-a9dd-116a07546832",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. Compare the training and validation performance (e.g., accuracy, loss) between the models with and without batch normalization.\n",
    "\n",
    "ANS- Compare the accuracy and loss metrics obtained from training the model with and without batch normalization. Analyze whether batch \n",
    "     normalization improved the model's performance in terms of convergence, generalization, or any other evaluation metrics.\n",
    "\n",
    "    \n",
    "Q6. Discuss the impact of batch normalization on the training process and the performance of the neural network.\n",
    "\n",
    "ANS- Discuss the observed impact of batch normalization on the training process and the performance of the neural network. Consider \n",
    "     factors such as training speed, convergence behavior, stability, generalization to unseen data, and any improvements in the \n",
    "     performance metrics compared to the model without batch normalization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b00f48ce-0736-4309-bec0-7e4ed2f29b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "3. Experimentation and Anaysis\n",
    "\n",
    "Q1. Experiment with different batch sizes and observe the effect on the training dynamics and model performance.\n",
    "\n",
    "ANS- Batch size refers to the number of training examples used in a single forward and backward pass during each iteration of training. \n",
    "     \n",
    "Here are the steps to experiment with different batch sizes and observe their effects:\n",
    "\n",
    "1. Choose a range of batch sizes to experiment with, such as 16, 32, 64, and 128.\n",
    "2. Train your neural network model using each batch size, keeping other hyperparameters consistent.\n",
    "3. Monitor and record the training dynamics, including the training loss, validation loss, and accuracy, for each batch size.\n",
    "4. Analyze the training dynamics and observe the convergence speed, stability, and generalization of the model for different batch sizes.\n",
    "5. Compare the performance metrics across different batch sizes and identify any trends or patterns.\n",
    "6. Consider factors such as training time, memory usage, and computational efficiency while interpreting the results.\n",
    "7. Based on the observations, make conclusions about the impact of batch size on the training dynamics and model performance.\n",
    "\n",
    "\n",
    "\n",
    "Q2. Discuss the advantages and potential limitations of batch normalization in improving the training of neural networks.\n",
    "\n",
    "ANS- Advantages of batch normalization:\n",
    "\n",
    "1. Improved training stability: Batch normalization reduces the effects of internal covariate shift, making the training process more \n",
    "                                stable and less sensitive to the initial weight initialization.\n",
    "2. Accelerated convergence: By normalizing the inputs within each mini-batch, batch normalization helps the model converge faster, leading \n",
    "                            to quicker training.\n",
    "3. Regularization effect: Batch normalization introduces a slight regularization effect, reducing the need for other regularization \n",
    "                          techniques like dropout or weight decay.\n",
    "4. Robustness to hyperparameters: Models with batch normalization are often more robust to the choice of learning rate and weight \n",
    "                                  initialization, making hyperparameter tuning less critical.\n",
    "5. Generalization to unseen data: Batch normalization helps the model generalize better to unseen data by reducing the reliance on \n",
    "                                  specific training examples, thus improving the model's ability to generalize.\n",
    "\n",
    "\n",
    "Limitations and considerations of batch normalization:\n",
    "\n",
    "1. Batch size dependency: Batch normalization can be sensitive to the choice of batch size. Smaller batch sizes might lead to increased \n",
    "                          noise in the estimation of mean and variance, impacting the effectiveness of normalization.\n",
    "2. Test-time behavior: During inference, batch normalization uses a running mean and variance computed during training. This might not \n",
    "                       accurately represent the statistics of the test data, leading to a discrepancy between training and testing \n",
    "                       performance.\n",
    "3. Additional computational overhead: Batch normalization introduces additional computations during training, which can slightly increase \n",
    "                                      the training time and memory requirements.\n",
    "\n",
    "\n",
    "Overall, batch normalization is a powerful technique that provides significant benefits in training neural networks. \n",
    "However, it is essential to consider the choice of batch size, test-time behavior, and potential computational overhead while \n",
    "utilizing batch normalization effectively. Experimentation and careful analysis are crucial to understanding its impact on specific \n",
    "models and datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1283e719-ef98-407e-a23f-dbe1f64d4689",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1250bc8-f5fb-4a18-a973-d74c3f6bb11b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f47bbb7-d222-4602-8b6a-7eca78d569b7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c6d71a-214f-47b3-9b2d-263e59f688df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
